<?wh
// command: convert-to-postgresql [--dryrun] [--nowait]
// short: Convert WHDB to postgresql

/*
    to estimate and test first:
    wh convert-to-postgresql --dryrun

    run conversion:
    wh convert-to-postgresql

    fully automatic conversion (use on your own risk!)
    wh db setserver readonly && wh convert-to-postgresql && sv restart webhare


    tools to help you with broken databases:
     - wh run mod::webhare_testsuite/scripts/runonce/fixdevserver.whscr
     - wh run mod::system/scripts/tools/fix-invalid-whfs-names.whscr
*/

LOADLIB "wh::async.whlib";
LOADLIB "wh::datetime.whlib";
LOADLIB "wh::files.whlib";
LOADLIB "wh::float.whlib";
LOADLIB "wh::ipc.whlib";
LOADLIB "wh::os.whlib";
LOADLIB "wh::promise.whlib";
LOADLIB "wh::dbase/dynquery.whlib";
LOADLIB "wh::dbase/postgresql.whlib";
LOADLIB "wh::dbase/whdb.whlib";
LOADLIB "wh::util/algorithms.whlib";
LOADLIB "wh::xml/dom.whlib";

LOADLIB "mod::system/lib/cluster.whlib";
LOADLIB "mod::system/lib/configure.whlib";
LOADLIB "mod::system/lib/database.whlib";
LOADLIB "mod::system/lib/resources.whlib";
LOADLIB "mod::system/lib/internal/modulemanager.whlib";
LOADLIB "mod::system/lib/internal/dbase/parser.whlib";
LOADLIB "mod::system/lib/internal/dbase/postgresql.whlib";
LOADLIB "mod::system/lib/internal/dbase/postgresql-blobhandling.whlib";
LOADLIB "mod::system/lib/internal/dbase/postgresql-bootstrap.whlib";
LOADLIB "mod::system/lib/internal/dbase/updatecommands.whlib";

LOADLIB "mod::wrd/lib/internal/support.whlib";



SCHEMA
< TABLE
  < BLOB id
  > "blob"
, TABLE
  < INTEGER id
  , STRING type
  , STRING tablename
  , INTEGER primarykey
  , STRING olddata __ATTRIBUTES__(BINARY)
  , STRING newdata
  > postgresql_migration_issues
> webhare_internal;

MACRO __BeginWHDBWork(INTEGER trans) __ATTRIBUTES__(EXTERNAL, EXECUTESHARESCRIPT);
MACRO __RollbackWHDBWork(INTEGER trans) __ATTRIBUTES__(EXTERNAL, EXECUTESHARESCRIPT);

PUBLIC RECORD ARRAY FUNCTION UpdateMultipleBlobs(RECORD blobconfig, RECORD ARRAY blobs)
{
  OBJECT blobresolver := GetBlobResolverByConfig(blobconfig);
  OBJECT blobhandler := NEW WHPostgreSQLBlobHandler(blobresolver);

  FOREVERY (RECORD rec FROM blobs)
  {
    STRING postgresqlid := blobhandler->ImportHardlinkableFile(rec.whdb_file);
    INSERT CELL postgresqlid := postgresqlid INTO blobs[#rec];
  }

  RETURN blobs;
}

IF (IsWithinFunctionRunningJob())
  RETURN;


RECORD args := ParseArguments(GetConsoleArguments(),
    [ [ name := "dryrun", type := "switch" ]
    , [ name := "nowait", type := "switch" ]
    ]);

IF (NOT RecordExists(args))
{
  Print("Syntax: wh run mod::system/scripts/database/migration.whscr [--dryrun] [--nowait]");
  TerminateScriptWithError(`Invalid arguments`);
}

DATETIME startconversion := GetCurrentDatetime();
LogInfo("system:convert-to-postgresql", "Conversion to postgresql starting" || (args.dryrun ? " (dry-run)" : ""), CELL[ type := "posgresql conversion", ...args ]);

MACRO __PGSQL_SETUPLOADEDBLOBINTERNALID(INTEGER transaction, BLOB blb, STRING blobid) __ATTRIBUTES__(EXTERNAL "wh_pgsql", EXECUTESHARESCRIPT);


ASYNC FUNCTION HandleProcess(OBJECT itr)
{
  INTEGER exitcode := -1;
  WHILE (TRUE)
  {
    RECORD rec := AWAIT itr->Next();
    IF (rec.done)
      BREAK;
    SWITCH (rec.value.type)
    {
      CASE "output" { PRINT(rec.value.line || "\n"); }
      CASE "error"  { PRINT(rec.value.line || "\n"); }
      CASE "close"  { PRINT(`Process closed with exit code ${rec.value.exitcode}\n`); exitcode := rec.value.exitcode; }
    }
  }
  RETURN exitcode;
}

/** Calculate the path of a whdb blob parent folder
    @param blobroot Root of the blob folder
    @param blobid Id of the blob
    @return Path to the blob storage location
*/
PUBLIC STRING FUNCTION GetWHDBBlobDir(STRING blobroot, INTEGER blobid)
{
  INTEGER mainfolder := blobid BITRSHIFT 24;
  INTEGER subfolder := (blobid BITRSHIFT 12) BITAND 4095;

  RETURN `${blobroot}/${mainfolder = 0 ? "blob" : `blob-${mainfolder}`}/${subfolder}/`;
}

STRING basedataroot := GetWebHareConfiguration().basedataroot;
STRING dbasefolder := MergePath(basedataroot, "dbase");
STRING postgresql_migrate_dbasefolder := MergePath(basedataroot, "postgresql-migration");
STRING postgresql_final_dbasefolder := MergePath(basedataroot, "postgresql");

IF (GetEnvironmentVariable("__WEBHARE_DBASE") = "postgresql")
  TerminateScriptWithError(`Current database is already set to 'postgresql'`);

IF (RecordExists(GetDiskFileProperties(postgresql_final_dbasefolder)))
  TerminateScriptWithError(`Existing PostgreSQL database found in '${postgresql_final_dbasefolder}', refusing to re-migrate`);

STRING want_readonly := IsDatabaseWritable() ? "" : "already_readonly";
IF(NOT args.dryrun)
{
  WHILE (want_readonly NOT IN [ "yes", "no", "already_readonly" ])
  {
    PRINT("Do you want to switch the WebHare database to readonly during migration? Please type 'yes' or 'no'.");
    want_readonly := ReadLineFrom(0, 4096, TRUE);
  }
}

IF(NOT args.dryrun AND NOT args.nowait)
{
  Print("Starting migration from WHDB to PostgreSQL....\n");
  Sleep(2000);
}

OBJECT trans := OpenPrimary();

IF (want_readonly = "yes")
{
  OBJECT proc := CreateProcess(MergePath(GetEnvironmentVariable("WEBHARE_DIR"), "bin/wh"), [ "db", "setserver", "readonly"],
      [ take_input :=       TRUE
      , take_output :=      TRUE
      , take_errors :=      TRUE
      ]);
  proc->Start();
  IF (WaitForPromise(HandleProcess(MakeProcessAsyncIterator(proc))) != 0)
    TerminateScriptWithError(`Could not switch the database to read-only mode`);

  __BeginWHDBWork(trans->id);
  INTEGER ARRAY transactionids := SELECT AS INTEGER ARRAY transid FROM __SendWHDBCommand(trans->id, 'show transactions');
  __RollbackWHDBWork(trans->id);

  INTEGER ARRAY org_transactionids := transactionids;
  WHILE (ArraysIntersect(org_transactionids, transactionids))
  {
    __BeginWHDBWork(trans->id);
    transactionids := SELECT AS INTEGER ARRAY transid FROM __SendWHDBCommand(trans->id, 'show transactions');
    __RollbackWHDBWork(trans->id);
    Sleep(100);
  }
}

// Start postgresql server
RECORD ARRAY postgresql_env :=
    [ [ name := "__WEBHARE_DBASE", value := "postgresql" ]
    , [ name := "WEBHARE_DBASENAME", value := "webhare" ]
    , [ name := "WEBHARE_POSTGRESQL_MIGRATION", value := "1" ]
    ] CONCAT
    SELECT *
      FROM GetEnvironment()
     WHERE name NOT IN [ "__WEBHARE_DBASE", "WEBHARE_POSTGRESQL_MIGRATION" ];

OBJECT psql_server := CreateProcess(
                              MergePath(GetEnvironmentVariable("WEBHARE_DIR"), "bin/dbserver.sh"),
                              STRING[],
                              [ take_input :=       FALSE
                              , take_output :=      FALSE
                              , take_errors :=      FALSE
                              ]);

psql_server->share_stdout := TRUE;
psql_server->share_stderr := TRUE;

psql_server->SetEnvironment(postgresql_env);
psql_server->Start();
OBJECT processitr := MakeProcessAsyncIterator(psql_server);
OBJECT processwait := HandleProcess(processitr);


OBJECT postgresql_trans;

// Get a blob handler that uses the migration folder for local disk storage
BLOB blobconfigdatablob := GetDiskResource(MergePath(GetEnvironmentVariable("WEBHARE_DATAROOT"), "etc/blobstorage.config.json"), [ allowmissing := TRUE ]);
RECORD blobconfig;
IF (LENGTH(blobconfigdatablob) != 0)
  blobconfig := DecodeJSONBlob(blobconfigdatablob);
blobconfig := blobconfig ?? [ type := "local-disk" ];
IF (blobconfig.type = "local-disk")
{
  IF (NOT CellExists(blobconfig, "blobfolder"))
    INSERT CELL blobfolder := postgresql_migrate_dbasefolder INTO blobconfig;
  ELSE
    blobconfig.blobfolder := Substitute(blobconfig.blobfolder, postgresql_final_dbasefolder, postgresql_migrate_dbasefolder);
}

OBJECT blobresolver := GetBlobResolverByConfig(blobconfig);
OBJECT blobhandler := NEW WHPostgreSQLBlobHandler(blobresolver);

INTEGER loop;
FOR (loop := 0; loop < 180; loop := loop + 1)
{
  postgresql_trans := __StartWHPostgreSQLTransaction(CELL
    [ dbasefolder :=        postgresql_migrate_dbasefolder
    , webhare_dbasename :=  "webhare"
    , blobhandler
    ]);
  IF (ObjectExists(postgresql_trans))
    BREAK;

  IF ((loop % 10) = 0)
    PRINT(`Waiting for PostgreSQL server to start\n`);
  WaitUntil(DEFAULT RECORD, AddTimeToDate(1000, GetCurrentDateTime()));
}

// Throw on errors during commit, so we don't need to check all commits
postgresql_trans->throwoncommiterror := TRUE;

webhare_internal := BindTransactionToSchema(postgresql_trans->id, "webhare_internal");

BootstrapPostgreSQL(postgresql_trans);

RECORD ARRAY schemas := trans->GetSchemaListing();

RECORD ARRAY apply_schemadefs;

FOREVERY (RECORD schemarec FROM SELECT * FROM schemas ORDER BY ToUppercase(schema_name) != "SYSTEM")
{
  IF (schemarec.is_system_schema)
    CONTINUE;

  STRING modulename := ToLowercase(schemarec.schema_name);

  PRINT(`Reading schema structure: ${modulename}\n`);

  STRING schemadef_str := GetSchemaModuleDef(GetPrimary(), schemarec.schema_name);

  OBJECT doc := MakeXMLDocument(StringToBlob(`<container xmlns="http://www.webhare.net/xmlns/system/moduledefinition">${schemadef_str}</container>`));
  OBJECT dbschema := doc->documentelement->GetElementsByTagNameNS("http://www.webhare.net/xmlns/system/moduledefinition", "databaseschema")->GetCurrentElements()[0];

  // Parse it back into a schema definition record
  RECORD parsed := ParseWHDBSchemaSpec(modulename, dbschema);

  // Apply bytea columns from the moduledefinitions
  IF (modulename IN GetInstalledModuleNames())
  {
    RECORD moduledef := GetModuleDatabaseSchema(modulename);
    IF (RecordExists(moduledef))
    {
      STRING ARRAY byteacols, longkeycols;
      FOREVERY (RECORD tbldef FROM moduledef.tables)
        FOREVERY (RECORD coldef FROM tbldef.cols)
          IF (coldef.dbtype = "BYTEA")
            INSERT `${tbldef.name}.${coldef.name}` INTO byteacols AT END;
          ELSE IF (coldef.dbtype = "__LONGKEY")
            INSERT `${tbldef.name}.${coldef.name}` INTO longkeycols AT END;

      FOREVERY (RECORD tbldef FROM parsed.tables)
      {
        FOREVERY (RECORD coldef FROM tbldef.cols)
        {
          IF (`${tbldef.name}.${coldef.name}` IN byteacols)
            parsed.tables[#tbldef].cols[#coldef].dbtype := "BYTEA";
          IF (`${tbldef.name}.${coldef.name}` IN longkeycols)
            parsed.tables[#tbldef].cols[#coldef].dbtype := "__LONGKEY";
        }
      }
    }
  }

  // Read access managers are not supported anymore, write access only for system.sites and system.fs_objects
  FOREVERY (RECORD tbldef FROM parsed.tables)
  {
    IF (tbldef.legacy_readaccessmgr != "")
    {
      PRINT(`Ignore read access manager '${tbldef.legacy_readaccessmgr}' on ${ToLowercase(schemarec.schema_name)}.${tbldef.name}\n`);
      parsed.tables[#tbldef].legacy_readaccessmgr := "";
    }
    IF (tbldef.legacy_writeaccessmgr != "" AND ToLowercase(`${schemarec.schema_name}.${tbldef.name}`) NOT IN [ "system.sites", "system.fs_objects"])
    {
      PRINT(`Ignore write access manager '${tbldef.legacy_writeaccessmgr}' on ${ToLowercase(schemarec.schema_name)}.${tbldef.name}\n`);
      parsed.tables[#tbldef].legacy_writeaccessmgr := "";
    }
  }


  INSERT parsed INTO apply_schemadefs AT END;
}


IF (want_readonly NOT IN [ "yes", "already_readonly"] )
  trans->BeginWork();
postgresql_trans->BeginWork();

PRINT(`Creating schemas... `);
RECORD cmd := GenerateIndependentSQLCommands(apply_schemadefs, postgresql_trans);
ExecuteSQLUpdates(postgresql_trans, cmd.commands);
PRINT(`done\n`);

WaitUntil(DEFAULT RECORD, GetCurrentDateTime());

PRINT(`Creating tables... `);
cmd := GeneratePostgreSQLDependentSQLCommands(apply_schemadefs, postgresql_trans, [ skipcreateforeignkeys := TRUE, skipcreateindices := TRUE ]);
ExecuteSQLUpdates(postgresql_trans, cmd.commands);
PRINT(`done\n`);

__LegacyCreateTable(postgresql_trans, "webhare_internal", "postgresql_migration_issues",
    [ primarykey := "id"
    , cols :=       [ [ column_name := "id", data_type := "INTEGER", autonumber_start := 1 ]
                    , [ column_name := "type", data_type := "VARCHAR", character_octet_length := 256 ]
                    , [ column_name := "tablename", data_type := "VARCHAR", character_octet_length := 256 ]
                    , [ column_name := "primarykey", data_type := "INTEGER" ]
                    , [ column_name := "olddata", data_type := "BYTEA" ] // maxlength not needed in PostgreSQL
                    , [ column_name := "newdata", data_type := "STRING", character_octet_length := 4096 ]
                    ]
    ]);

// Defer all constraints
postgresql_trans->__ExecSQL("SET CONSTRAINTS ALL DEFERRED");

__BeginWHDBWork(trans->id);
RECORD ARRAY indices := SELECT tab := COLUMN "table", recs := MAX(totalrecords) FROM __SendWHDBCommand(trans->id, "SHOW INDICES") GROUP BY COLUMN"table";
__RollbackWHDBWork(trans->id);
INTEGER64 totalnumrows := SELECT AS INTEGER64 SUM(recs) FROM indices;

PRINT(`Starting record import\n`);

WaitUntil(DEFAULT RECORD, GetCurrentDateTime());

INTEGER64 lastprogress, curdatatransferred, lastdatatransferred,donerows;
INTEGER64 lastblobsize;
STRING lastprinted;
DATETIME lastset;
MACRO AddProgress(STRING tablename, INTEGER64 nr, INTEGER64 rowsintable)
{
  Print(`\rProcessing ${tablename}... `);
  INTEGER64 prevlastprogress := lastprogress < 0 ? 0i64 : lastprogress;
  IF (nr >= 0)
  {
    lastprogress := prevlastprogress + nr;
    donerows := donerows + nr;
  }
  ELSE
  {
    lastprogress := -1;
    lastdatatransferred := 0;
    curdatatransferred := 0;
  }
  STRING newnr := lastprogress >= 0 ? ToString(lastprogress) : "";
  DATETIME now := GetCurrentDateTime();
  IF (lastprogress >= 0 AND nr != 0)
  {
    INTEGER sofar := GetDateTimeDifference(startconversion, now).msecs;
    FLOAT overallprogress := FLOAT(donerows) / totalnumrows;

    INTEGER msecs := GetDateTimeDifference(lastset, now).msecs;
    INTEGER totalsec := GetDateTimeDifference(startconversion, now).msecs / 1000;
    IF (msecs > 0 AND rowsintable > 0)
      newnr := newnr || `/${rowsintable}, ${1000 * nr / msecs} records/second, table: ${FormatFloat((FLOAT(lastprogress) / rowsintable) * 100,2)}%, overall: ${FormatFloat(overallprogress * 100,2)}%, ${totalsec} secs`;
  }
  PRINT(newnr);
  INTEGER overlen := LENGTH(lastprinted) - LENGTH(newnr);
  IF (overlen > 0)
    PRINT(RepeatText(" ", overlen) || RepeatText("\x08", overlen));
  lastprinted := newnr;
  lastdatatransferred := curdatatransferred;
  lastset := now;
}


BOOLEAN got_invalid_records;

STRING ARRAY createdblobfolders;
RECORD ARRAY importedblobs;

ASYNC MACRO UploadAndAddNewBlobs(RECORD ARRAY new_blobs)
{
  INTEGER64 totalsize := SELECT AS INTEGER64 SUM(size) FROM new_blobs;
  DATETIME start := GetCurrentDateTime();

  // For local-disk, don't got async and running jobs
  RECORD ARRAY data := blobconfig.type = "local-disk"
      ? UpdateMultipleBlobs(blobconfig, new_blobs)
      : AWAIT AsyncCallFunctionFromJob(Resolve("#UpdateMultipleBlobs"), blobconfig, SELECT whdb_file FROM new_blobs);

  curdatatransferred := curdatatransferred + totalsize;

  RECORD ARRAY local_importedblobs := importedblobs;
  importedblobs := RECORD[];

  FOREVERY (RECORD rec FROM new_blobs)
  {
    RECORD blobrec := CELL
        [ rec.whdbid
        , postgresqlid :=     data[#rec].postgresqlid
        ];

    FOREVERY (BLOB blb FROM rec.blobs)
      __PGSQL_SETUPLOADEDBLOBINTERNALID(postgresql_trans->id, blb, blobrec.postgresqlid);

    RECORD ipos := RecordLowerBound(local_importedblobs, blobrec, [ "WHDBID" ]);
    INSERT blobrec INTO local_importedblobs AT ipos.position;

    INSERT [ id := rec.blobs[0] ] INTO webhare_internal."blob";
  }

  curdatatransferred := curdatatransferred + SELECT AS INTEGER64 SUM(size) FROM new_blobs;

  importedblobs := local_importedblobs;
}

BOOLEAN FUNCTION ProcessRecords(OBJECT inserter, STRING tablename, STRING ARRAY blobcols, STRING ARRAY varcharcols, RECORD ARRAY records, INTEGER64 rowsintable)
{
  records := SELECT AS RECORD ARRAY tbl FROM records;

  RECORD ARRAY new_blobs;

  // Check format of first record
  IF (LENGTH(records) != 0)
  {
    FOREVERY (RECORD rec FROM UnpackRecord(records[0]))
      IF (TypeID(rec.value) = TypeID(BLOB) AND ToLowercase(rec.name) NOT IN blobcols)
        ABORT(records[0], `Col ${rec.name} in is blob but not in blobcols`);
  }

  FOREVERY (RECORD rec FROM records)
  {
    FOREVERY (STRING col FROM blobcols)
    {
      BLOB data := GetCell(rec, col);
      IF (LENGTH(data) = 0)
        CONTINUE;

      INTEGER whdbid := __GetWHDBBlobInternalId(data);
      IF (whdbid = 0)
        ABORT(`No whdb blob id found in blob`);

      // Already imported?
      RECORD ipos := RecordLowerBound(importedblobs, CELL[ whdbid ], [ "WHDBID" ]);
      IF (ipos.found)
      {
        __PGSQL_SETUPLOADEDBLOBINTERNALID(postgresql_trans->id, data, importedblobs[ipos.position].postgresqlid);
        CONTINUE;
      }

      RECORD npos := RecordLowerBound(new_blobs, CELL[ whdbid ], [ "WHDBID" ]);
      IF (npos.found)
        INSERT data INTO new_blobs[npos.position].blobs AT END;
      ELSE
      {
        INSERT CELL
            [ whdbid
            , size := LENGTH64(data)
            , whdb_file := MergePath(GetWHDBBlobDir(dbasefolder, whdbid), ToString(whdbid))
            , blobs := [ data ]
            ] INTO new_blobs AT npos.position;
      }
    }
  }

  IF (blobconfig.type = "local-disk")
    WaitForPromise(UploadAndAddNewBlobs(new_blobs));
  ELSE
  {
    OBJECT serializer := MakeCallSerializer([ maxconcurrent := 8 ]);
    OBJECT ARRAY promises;
    FOR (INTEGER i := 0; i < LENGTH(new_blobs); i := i + 16)
    {
      RECORD ARRAY part := ArraySlice(new_blobs, i, 16);
      INSERT serializer->Call(PTR UploadAndAddNewBlobs, part) INTO promises AT END;
    }

    WaitForPromise(CreatePromiseAll(promises));
  }

  BOOLEAN allvalid := TRUE;
  FOREVERY (RECORD rec FROM records)
  {
    FOREVERY (STRING col FROM varcharcols)
    {
      STRING data := GetCell(rec, col);

      IF (NOT IsValidUTF8(data))
      {
        STRING newdata := DecodeInvalidUTF8Chars(data, "ISO-8859-15");

        RECORD registration := CELL
            [ tablename
            , type :=       "invalidutf8"
            , olddata :=    EncodeHSON(data)
            , newdata :=    newdata
            ];

        IF (CellExists(rec, "id") AND TypeID(rec.id) = TypeID(INTEGER))
          INSERT CELL primarykey := rec.id INTO registration;

        INSERT registration INTO webhare_internal.postgresql_migration_issues;

        // Update the new data
        records[#rec] := CellUpdate(records[#rec], col, newdata);

        PRINT(`\n invalid utf-8 in column '${col}': ${Substring(EncodeHSON(GetCell(rec, col)),5)}\n`); //use hson so we see ALL invalid characters, but strip the hson: part
        IF (NOT IsValidUTF8(newdata))
        {
          PRINT(`** Could not correct!\n`);
          allvalid := FALSE;
        }
        ELSE
        {
          PRINT(` converted to: ${EncodeJava(newdata)}\n`);
        }
      }
    }
  }

  IF (allvalid)
    inserter->InsertRecords(records);
  ELSE
    got_invalid_records := TRUE;

  AddProgress(tablename, LENGTH(records), rowsintable);
  WaitUntil(DEFAULT RECORD, GetCurrentDateTime());
  RETURN TRUE;
}

FOREVERY (RECORD schemadef FROM apply_schemadefs)
{
  FOREVERY (RECORD tabledef FROM schemadef.tables)
  {
    STRING tblname := `${schemadef.name}.${tabledef.name}`;
    INTEGER64 rowsintable := SELECT AS INTEGER64 recs FROM indices WHERE Touppercase(tab) = ToUppercase(tblname);
    PRINT(`Processing ${tblname}... `);
    STRING ARRAY cols := SELECT AS STRING ARRAY name FROM tabledef.cols WHERE internalcolumnname = "";
    STRING ARRAY blobcols := SELECT AS STRING ARRAY name FROM tabledef.cols WHERE internalcolumnname = "" AND dbtype = "BLOB";
    STRING ARRAY varcharcols := SELECT AS STRING ARRAY name FROM tabledef.cols WHERE internalcolumnname = "" AND dbtype = "VARCHAR";

    OBJECT query := NEW DynamicQuery;
    query->AddTable("tbl", trans->id, `${schemadef.name}.${tabledef.name}`, cols);

    OBJECT inserter := GetDynamicInserter(postgresql_trans, `${schemadef.name}.${tabledef.name}`, CELL[ cols ]);

    AddProgress(tblname, -1, rowsintable);
    query->ExecuteTo(PTR ProcessRecords(inserter, tblname, blobcols, varcharcols, #1, rowsintable));
    INTEGER64 inserted := lastprogress < 0 ? 0i64 : lastprogress;
    AddProgress(tblname, -1, rowsintable);
    PRINT(`done: ${inserted} records\n`);
  }
}

IF (got_invalid_records)
  TerminateScriptWithError(`Got UTF-8 invalid records, migration failed`);

PRINT(`Checking all deferred constraints... `);
postgresql_trans->__ExecSQL("SET CONSTRAINTS ALL IMMEDIATE");
PRINT(`done\n`);

PRINT(`Creating indices and foreign keys, this may take a few minutes... `);
cmd := GeneratePostgreSQLDependentSQLCommands(apply_schemadefs, postgresql_trans);
ExecuteSQLUpdates(postgresql_trans, cmd.commands);
PRINT(`done\n`);

/** Renames all system_rights object-tables to the name they will have using PostgreSQL table oids
*/
MACRO RenameRightsTables()
{
  SCHEMA postgresql_system LIKE system := BindTransactionToSchema(postgresql_trans->id, "system");

  FOREVERY (RECORD objtype FROM SELECT names := GroupedValues(name), tablename, storagetable FROM system.module_objecttypes WHERE tablename != "" GROUP BY tablename, storagetable)
  {
    STRING obj_tablename := objtype.tablename;
    STRING storagetable_localname := Tokenize(objtype.storagetable, ".")[1];

    INTEGER dpos := SearchSubString(obj_tablename, ".");
    STRING t_schema := LEFT(obj_tablename, dpos);
    IF (t_schema = "")
      t_schema := "PUBLIC";
    STRING t_table := SubString(obj_tablename, dpos + 1);

    RECORD tablerec :=
        SELECT object_id
             , primary_key_name
          FROM postgresql_trans->GetTableListing(t_schema)
         WHERE ToUppercase(table_name) = ToUppercase(t_table);

    IF (NOT RecordExists(tablerec))
      ABORT("Can't find referenced table '"||objtype.tablename||"' for object types " || Detokenize(objtype.names,','));

    STRING new_storagetable_localname := "o_" || tablerec.object_id;

    PRINT(`Rename "system_rights".${PostgreSQLEscapeIdentifier(storagetable_localname)} to ${PostgreSQLEscapeIdentifier(new_storagetable_localname)}\n`);

    postgresql_trans->__ExecSQL(`ALTER TABLE "system_rights".${PostgreSQLEscapeIdentifier(storagetable_localname)} RENAME TO ${PostgreSQLEscapeIdentifier(new_storagetable_localname)}`);

    UPDATE postgresql_system.module_objecttypes
       SET storagetable := `system_rights.${new_storagetable_localname}`
     WHERE tablename = objtype.tablename;
  }
}

PRINT(`Renaming rights tables...\n`);
RenameRightsTables();
PRINT(`done\n`);

PRINT("Committing\n");
postgresql_trans->CommitWork();

PRINT("Shutting down PostgreSQL\n");
psql_server->SendInterrupt();
WaitUntil([ promise := processwait ], MAX_DATETIME);

PRINT("Sync to disk\n");
OBJECT sync_process := CreateProcess("/usr/bin/sync", STRING[],
    [ take_output := TRUE
    , take_errors := TRUE
    , merge_output_errors := TRUE
    ]);
sync_process->Start();
WaitForPromise(HandleProcess(MakeProcessAsyncIterator(sync_process)));

IF (NOT args.dryrun)
{
  PRINT(`Moving PostgreSQL database into expected location\n`);
  IF (NOT MoveDiskPath(postgresql_migrate_dbasefolder, postgresql_final_dbasefolder))
    TerminateScriptWithError(`Could not rename dbase folder from '${postgresql_migrate_dbasefolder}' to '${postgresql_final_dbasefolder}'`);

  PRINT(`Migration complete, please restart WebHare\n`);
}

INTEGER totaltime := GetDateTimeDifference(startconversion, GetCurrentDatetime()).msecs;
STRING finaltext := "Conversion to postgresql finished" || (args.dryrun ? " (dry-run)" : "")|| " in " || totaltime/1000 || " seconds";
LogInfo("system:convert-to-postgresql", finaltext, CELL[ type := "posgresql conversion", ...args ]);
